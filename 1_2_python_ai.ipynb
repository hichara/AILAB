{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AI absolute basics using Python\n",
    "\n",
    "based on material from Joel Grus, adapted by Heiko RÃ¶lke"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62789d7ca2271f97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have learned ourselves some Python and libraries, we will look at some very basic (read: boring) AI stuff. Later, in hindsight, it will be surprising how simple the basic building blocks of Deep Learning are."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8818dd6cde9eb7af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some simple loss function\n",
    "\n",
    "def sum_of_squares(vect):\n",
    "    return sum([x**2 for x in vect])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are looking for the input vector \"vect\" that minimizes the return result.\n",
    "\n",
    "I know, this is dead easy to come up with, but let's assume we don't know the correct answer.\n",
    "What can we do?\n",
    "\n",
    "We choose a random start point (vector) and look for the *gradient*, the vector of partial derivatives. The gradient tells us in which direction to go to come closer to a maximum - and therefor also to a minimum. We choose a step width, apply this step to the input vector and start anew. Until the return value does not change (much) anymore - then we have reached a minimum. Possibly a local one... "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d41356eb382c8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's further assume that we also do not know the derivative function of our loss function. To come up with an approximation, we use the asymptotic difference quotient - you might recall this from math in school or university(?)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0058a90d73ce715"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def difference_quotient(func, x, h):\n",
    "    return (func(x+h) - func(x)) / h"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62cfdc8d307000a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can easily test this for a simple example, to see that we can come reasonably close to the actual values:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "211ff971b471103e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "# you might recall that the derivative of x^2 is 2x\n",
    "def derivative(x):\n",
    "    return 2*x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d918ef1c552dbe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xs = range(-10,11)\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "plt.plot(xs, actuals, \"rx\", label=\"Actual\")\n",
    "plt.plot(xs, estimates, \"b+\", label=\"Estimate\")\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "461a2c036b7acbfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fits reasonably well...\n",
    "No Proof! But some reassurance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6df3dfe8ebf9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# i-th partial \"derivative\" of func over input vect with distance h \n",
    "def partial_difference_quotient(func, vect, i, h):\n",
    "    w = [v_j + (h if j==i else 0) for j, v_j in enumerate(vect)]\n",
    "    return (func(w) - func(vect)) / h\n",
    "\n",
    "\n",
    "# estimate gradient using partial difference quotient\n",
    "def estimate_gradient(func, vect, h=0.0001):\n",
    "    return np.array([partial_difference_quotient(func, vect, i, h) for i in range(len(vect))])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d002499a73dbbdc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now want to put this into practice. We use a three-dimensional vector for the sum-of-squares function above and want to find the minimum - all zeros.\n",
    "\n",
    "For that, we choose a random starting vector, calculate the gradient and make a small step in the opposite direction. This is repeated for a fixed amount of steps (or until the gradient becomes very small)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a16ac160ff8f50d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gradient_step(vect, grad, step_size):\n",
    "    step = step_size * grad\n",
    "    return vect + step\n",
    "\n",
    "\n",
    "# choose random starting point\n",
    "vect = np.random.uniform(-10,10,3)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    grad = estimate_gradient(sum_of_squares, vect) \n",
    "    vect = gradient_step(vect, grad, -0.01)\n",
    "    print(epoch, vect)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e15edb117155fed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some remarks on efficiency\n",
    "\n",
    "The method we learned here works for nearly every case (except some really weird loss functions). \n",
    "\n",
    "But: Those of you concerned about code efficiency (I guess all of you) might have noticed that we introduced a lot of calculations. For each gradient step and a vector of length n we have to calculate the loss function 2n times. Thats a lot of work considering modern LLMs do have billions of parameters (= loss function calculations per step) and are trained \"countless\" times (think number of inputs (billions to trillions of tokens) times training steps (easily hundreds to thousands of so-called \"epochs\").\n",
    "\n",
    "There are some simple and many not so simple optimisations. Unfortunately, we cannot introduce them here. But maybe you can come up with some suggestions?  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e51a2356b59c832a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## From here on: (mostly) differentiable functions\n",
    "\n",
    "In ML, especially DL, we prefer loss functions that are differentiable, so that we do not have to use the difference quotient each time. But be assured this would be possible, as you can easily try out for yourself. \n",
    "\n",
    "Sometimes we will use functions that are not differentiable, or at least not everywhere. This will be clearly stated."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28657f6d21946379"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An application example: fitting functions on data\n",
    "\n",
    "In practice, we of have data and want to find a function fitting this data. \n",
    "\n",
    "A simple example: We have some data points and suspect that a linear function could fit the data. We want to find the parameters of that function (slope, intercept) so that the error - the distance of data points to the function - is minimized.\n",
    "\n",
    "For the error function, we use the mean quadratic distance of each data point to the function approximation. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a29b75b7499096e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate data for the linear function f(x) = 20*x + 5\n",
    "inputs = np.array([(x, 20*x+5) for x in range(-50,50)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "407f93031aab5681"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gradient method for linear function approximation\n",
    "def linear_gradient(x, y, theta):  # theta is the vector of the (current estimates of the) function coefficients\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept\n",
    "    error = predicted - y\n",
    "    grad_slope = 2 * error * x  # \"cheated\", as explained above - this is the partial derivation of the error function for the slope\n",
    "    grad_intercept = 2 * error   # and for the intercept (no x)\n",
    "    return np.array([grad_slope, grad_intercept])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22414adbf889c86b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "theta = np.random.uniform(-1,1,2)  # random values as a starter\n",
    "\n",
    "learning_rate = 0.001  # a new name for the value h, the step size\n",
    "\n",
    "for epoch in range(5000):\n",
    "    grad = np.mean(np.array([linear_gradient(x, y, theta) for x, y in inputs]), axis=0)\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    print(epoch, theta)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46c5993f518f8c69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batch or Minibatch?\n",
    "\n",
    "What we did so far was to use all data in all steps. Feasible for our toy example, but impractical for larger problems.\n",
    "\n",
    "One solution is to split the complete data batch to so-called mini-batches and train on those:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b59c80a03d82c522"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def minibatches(dataset, batch_size, shuffle = True):\n",
    "    # Start index 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "    if shuffle: random.shuffle(batch_starts) # shuffle batches\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "554df1b00696f39f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Gradient Descent using Minibatches\n",
    "theta = np.random.uniform(-1,1,2)  # random values as a starter\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = np.mean(np.array([linear_gradient(x, y, theta) for x, y in batch]), axis=0)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "        print(epoch, theta)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aa9ef5f10ce9e06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Minibatches to the extremes: Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Why not using smaller minibatches? Batch size 1 might be sufficient..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4095b34adbd6d1a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "theta = np.random.uniform(-1,1,2)  # random values as a starter\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "        print(epoch, theta)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85c724a5976ddc7c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this example, the SGD is faster than the other methods, as calculating the mean error is bad for fast approximation. For other, less perfect circumstances this might be the other way round. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "192f40ece16620a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises\n",
    "\n",
    "Before we go to the next section, we want to play around with the concepts above.\n",
    "\n",
    "1. For the code up to the cell with the \"gradient_step\" function. Make sure the gradient descent code is working for you. Play around with:\n",
    "    * step sizes\n",
    "    * functions\n",
    "    * number of epochs\n",
    "    * ...\n",
    "2. Visualize the learning process\n",
    "    * for the original code\n",
    "    * for your experiments\n",
    "3. Have a look at the function fitting example and make sure it works for you as well.\n",
    "    * Change the function parameter, generate new data and find a fitting function for that.\n",
    "    * *Advanced*: Change the example to a non-linear function, e.g. a quadratic (or higher) function.\n",
    "    * Visualize your data and (some of the) approximated functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23cf277fd4545ed3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
